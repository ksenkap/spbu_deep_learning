{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"8eZ7gyuwPKF-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734043935399,"user_tz":-180,"elapsed":2797,"user":{"displayName":"Ксения Павлова","userId":"07704179812315976489"}},"outputId":"9380115a-7bcf-47b1-9f53-35c8bb296b17"},"id":"8eZ7gyuwPKF-","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","id":"a4eda323-3063-435f-bc1a-b1446b014c1c","metadata":{"id":"a4eda323-3063-435f-bc1a-b1446b014c1c"},"source":["# Простейшая рекуррентная сеть\n","В этом ноутбуке мы пройдемся по основам работы с RNN. Сегодня займемся задачей генерации текста."]},{"cell_type":"code","execution_count":2,"id":"70d8b089-5f9c-4dcb-8b14-3f565c24e438","metadata":{"id":"70d8b089-5f9c-4dcb-8b14-3f565c24e438","executionInfo":{"status":"ok","timestamp":1734043944076,"user_tz":-180,"elapsed":4160,"user":{"displayName":"Ксения Павлова","userId":"07704179812315976489"}}},"outputs":[],"source":["import warnings\n","from typing import Iterable, Tuple\n","import torch\n","from tqdm.notebook import tqdm\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from tqdm.notebook import tqdm\n","from IPython.display import clear_output\n","from torch.utils.data import Dataset, DataLoader\n","from collections import Counter\n","from torch import nn\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","from torch.distributions.categorical import Categorical\n","\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","id":"198424b3-07c0-4b46-83f0-8bbb53acacd4","metadata":{"id":"198424b3-07c0-4b46-83f0-8bbb53acacd4"},"source":["В качестве обучающего датасета возьмем набор из 120 тысяч анекдотов на русском языке.\n","[Ссылка на данные](https://archive.org/download/120_tysyach_anekdotov) и [пост на хабре про тематическое моделирование](https://habr.com/ru/companies/otus/articles/723306/)"]},{"cell_type":"code","execution_count":3,"id":"b5fda8b3-2e4b-4385-aad5-b10ad73a5d35","metadata":{"id":"b5fda8b3-2e4b-4385-aad5-b10ad73a5d35","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1734043944076,"user_tz":-180,"elapsed":3,"user":{"displayName":"Ксения Павлова","userId":"07704179812315976489"}},"outputId":"312c2d50-7838-4db7-da3c-4f4b33c4ad81"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'|startoftext|>Друзья мои, чтобы соответствовать вам, я готов сделать над собой усилие и стать лучше. Но тогда и вы станьте немного хуже!\\n\\n<|startoftext|>- Люся, ты все еще хранишь мой подарок?- Да.- Я думал, ты выкинула все, что со мной связано.- Плюшевый мишка не виноват, что ты ебл@н...\\n\\n<|startoftext|>- А вот скажи честно, ты во сне храпишь?- Понятие не имею, вроде, нет. От со'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}],"source":["with open(r\"/content/drive/MyDrive/anek_djvu.txt\", \"r\", encoding=\"utf-8\") as f:\n","    text = f.read()\n","text[118:500]"]},{"cell_type":"markdown","id":"007f21a5-c7e3-445f-b902-24e18242bd7b","metadata":{"id":"007f21a5-c7e3-445f-b902-24e18242bd7b"},"source":["Мы не хотим моделировать все подряд, поэтому разобьем датасет на отдельные анекдоты.  "]},{"cell_type":"code","execution_count":4,"id":"fddd3f65-a156-4bbd-8c56-078652d38ac2","metadata":{"id":"fddd3f65-a156-4bbd-8c56-078652d38ac2","executionInfo":{"status":"ok","timestamp":1734043944409,"user_tz":-180,"elapsed":335,"user":{"displayName":"Ксения Павлова","userId":"07704179812315976489"}}},"outputs":[],"source":["def cut_data(text):\n","    return text.replace(\"\\n\\n\", \"\").split(\"<|startoftext|>\")[1:]"]},{"cell_type":"code","execution_count":5,"id":"3ae42013-ef71-485c-805e-8cc4c61fe6f7","metadata":{"id":"3ae42013-ef71-485c-805e-8cc4c61fe6f7","executionInfo":{"status":"ok","timestamp":1734043944847,"user_tz":-180,"elapsed":439,"user":{"displayName":"Ксения Павлова","userId":"07704179812315976489"}}},"outputs":[],"source":["cut_text = cut_data(text)"]},{"cell_type":"code","execution_count":6,"id":"67e8e214-e40c-4705-beb4-f51a6a284137","metadata":{"id":"67e8e214-e40c-4705-beb4-f51a6a284137","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734043944847,"user_tz":-180,"elapsed":2,"user":{"displayName":"Ксения Павлова","userId":"07704179812315976489"}},"outputId":"4441eace-9c36-42c4-e5bd-0d71736e2523"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Друзья мои, чтобы соответствовать вам, я готов сделать над собой усилие и стать лучше. Но тогда и вы станьте немного хуже!',\n"," '- Люся, ты все еще хранишь мой подарок?- Да.- Я думал, ты выкинула все, что со мной связано.- Плюшевый мишка не виноват, что ты ебл@н...',\n"," '- А вот скажи честно, ты во сне храпишь?- Понятие не имею, вроде, нет. От собственного храпа по крайней мере еще ни разу не просыпался.- Ну, так у жены спроси.- А жена и подавно не знает. У нее странная привычка после замужества возникла: как спать ложится - беруши вставляет.',\n"," 'Поссорилась с мужем. Пока он спал, я мысленно развелась с ним, поделила имущество, переехала, поняла, что жить без него не могу, дала последний шанс, вернулась. В итоге, ложусь спать уже счастливой женщиной.',\n"," 'Если тебя посещают мысли о смерти - это еще полбеды. Беда - это когда смерть посещают мысли о тебе...']"]},"metadata":{},"execution_count":6}],"source":["cut_text[1:6]"]},{"cell_type":"markdown","id":"282f6226-74c6-4488-a7bc-9360437e1b1f","metadata":{"id":"282f6226-74c6-4488-a7bc-9360437e1b1f"},"source":["Сделаем для начала самую простую модель с токенами на уровне символов. Это значит, что каждому символу в тексте ставится в соответствие некоторое число. Некоторые способы токенизации используют части слов или, наоборот, части бинарного представления текста."]},{"cell_type":"code","execution_count":7,"id":"3e923efb-a3d5-4e22-b8e0-8bf6260d1e71","metadata":{"id":"3e923efb-a3d5-4e22-b8e0-8bf6260d1e71","executionInfo":{"status":"ok","timestamp":1734043948127,"user_tz":-180,"elapsed":4,"user":{"displayName":"Ксения Павлова","userId":"07704179812315976489"}}},"outputs":[],"source":["unique_chars = tuple(set(text))\n","int2char = dict(enumerate(unique_chars))\n","char2int = {ch: ii for ii, ch in int2char.items()}"]},{"cell_type":"markdown","id":"f99fa447-208d-4285-bb76-0870e985ce58","metadata":{"id":"f99fa447-208d-4285-bb76-0870e985ce58"},"source":["Напишем функции для энкодинга и декодинга нашего текста. Они будут преобразовывать список символов в список чисел и обратно."]},{"cell_type":"code","source":["def encode(sentence, vocab):\n","    return [vocab[sys] for sys in sentence] # List of ints\n","\n","def decode(tokens, vocab):\n","    return \"\".join(vocab[toc] for toc in tokens)# list of strings"],"metadata":{"id":"Vv2wsq5vFgAk","executionInfo":{"status":"ok","timestamp":1734043948127,"user_tz":-180,"elapsed":3,"user":{"displayName":"Ксения Павлова","userId":"07704179812315976489"}}},"id":"Vv2wsq5vFgAk","execution_count":8,"outputs":[]},{"cell_type":"code","execution_count":9,"id":"9d0b4340-44bb-45ab-8cfe-972c9cfd410e","metadata":{"id":"9d0b4340-44bb-45ab-8cfe-972c9cfd410e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734043948577,"user_tz":-180,"elapsed":2,"user":{"displayName":"Ксения Павлова","userId":"07704179812315976489"}},"outputId":"b48446f6-568a-4ead-ca05-b6d2c67625df"},"outputs":[{"output_type":"stream","name":"stdout","text":["Исходная строка: - А вот скажи честно, ты во сне храпишь?- Понятие не имею, вроде, нет. От собственного храпа по крайней мере еще ни разу не просыпался.- Ну, так у жены спроси.- А жена и подавно не знает. У нее странная привычка после замужества возникла: как спать ложится - беруши вставляет.\n","Закодированная строка: [105, 35, 89, 35, 192, 51, 54, 35, 161, 12, 181, 62, 26, 35, 16, 97, 161, 54, 69, 51, 70, 35, 54, 180, 35, 192, 51, 35, 161, 69, 97, 35, 116, 185, 181, 45, 26, 36, 13, 136, 105, 35, 71, 51, 69, 176, 54, 26, 97, 35, 69, 97, 35, 26, 106, 97, 59, 70, 35, 192, 185, 51, 199, 97, 70, 35, 69, 97, 54, 127, 35, 43, 54, 35, 161, 51, 190, 161, 54, 192, 97, 69, 69, 51, 112, 51, 35, 116, 185, 181, 45, 181, 35, 45, 51, 35, 12, 185, 181, 73, 69, 97, 73, 35, 106, 97, 185, 97, 35, 97, 205, 97, 35, 69, 26, 35, 185, 181, 174, 120, 35, 69, 97, 35, 45, 185, 51, 161, 180, 45, 181, 207, 161, 176, 127, 105, 35, 210, 120, 70, 35, 54, 181, 12, 35, 120, 35, 62, 97, 69, 180, 35, 161, 45, 185, 51, 161, 26, 127, 105, 35, 89, 35, 62, 97, 69, 181, 35, 26, 35, 45, 51, 199, 181, 192, 69, 51, 35, 69, 97, 35, 174, 69, 181, 97, 54, 127, 35, 14, 35, 69, 97, 97, 35, 161, 54, 185, 181, 69, 69, 181, 176, 35, 45, 185, 26, 192, 180, 16, 12, 181, 35, 45, 51, 161, 207, 97, 35, 174, 181, 106, 120, 62, 97, 161, 54, 192, 181, 35, 192, 51, 174, 69, 26, 12, 207, 181, 109, 35, 12, 181, 12, 35, 161, 45, 181, 54, 13, 35, 207, 51, 62, 26, 54, 161, 176, 35, 105, 35, 190, 97, 185, 120, 36, 26, 35, 192, 161, 54, 181, 192, 207, 176, 97, 54, 127]\n","Декодированная строка: ['-', ' ', 'А', ' ', 'в', 'о', 'т', ' ', 'с', 'к', 'а', 'ж', 'и', ' ', 'ч', 'е', 'с', 'т', 'н', 'о', ',', ' ', 'т', 'ы', ' ', 'в', 'о', ' ', 'с', 'н', 'е', ' ', 'х', 'р', 'а', 'п', 'и', 'ш', 'ь', '?', '-', ' ', 'П', 'о', 'н', 'я', 'т', 'и', 'е', ' ', 'н', 'е', ' ', 'и', 'м', 'е', 'ю', ',', ' ', 'в', 'р', 'о', 'д', 'е', ',', ' ', 'н', 'е', 'т', '.', ' ', 'О', 'т', ' ', 'с', 'о', 'б', 'с', 'т', 'в', 'е', 'н', 'н', 'о', 'г', 'о', ' ', 'х', 'р', 'а', 'п', 'а', ' ', 'п', 'о', ' ', 'к', 'р', 'а', 'й', 'н', 'е', 'й', ' ', 'м', 'е', 'р', 'е', ' ', 'е', 'щ', 'е', ' ', 'н', 'и', ' ', 'р', 'а', 'з', 'у', ' ', 'н', 'е', ' ', 'п', 'р', 'о', 'с', 'ы', 'п', 'а', 'л', 'с', 'я', '.', '-', ' ', 'Н', 'у', ',', ' ', 'т', 'а', 'к', ' ', 'у', ' ', 'ж', 'е', 'н', 'ы', ' ', 'с', 'п', 'р', 'о', 'с', 'и', '.', '-', ' ', 'А', ' ', 'ж', 'е', 'н', 'а', ' ', 'и', ' ', 'п', 'о', 'д', 'а', 'в', 'н', 'о', ' ', 'н', 'е', ' ', 'з', 'н', 'а', 'е', 'т', '.', ' ', 'У', ' ', 'н', 'е', 'е', ' ', 'с', 'т', 'р', 'а', 'н', 'н', 'а', 'я', ' ', 'п', 'р', 'и', 'в', 'ы', 'ч', 'к', 'а', ' ', 'п', 'о', 'с', 'л', 'е', ' ', 'з', 'а', 'м', 'у', 'ж', 'е', 'с', 'т', 'в', 'а', ' ', 'в', 'о', 'з', 'н', 'и', 'к', 'л', 'а', ':', ' ', 'к', 'а', 'к', ' ', 'с', 'п', 'а', 'т', 'ь', ' ', 'л', 'о', 'ж', 'и', 'т', 'с', 'я', ' ', '-', ' ', 'б', 'е', 'р', 'у', 'ш', 'и', ' ', 'в', 'с', 'т', 'а', 'в', 'л', 'я', 'е', 'т', '.']\n"]}],"source":["sentence = cut_text[3]  # Берем первую строку из подготовленного текста\n","encoded_sentence = encode(sentence, char2int)\n","decoded_sentence = decode(encoded_sentence, int2char)\n","\n","print(\"Исходная строка:\", sentence)\n","print(\"Закодированная строка:\", encoded_sentence)\n","print(\"Декодированная строка:\", decoded_sentence)"]},{"cell_type":"markdown","id":"017baeba-1197-4d21-8cc8-28ccf43262c5","metadata":{"id":"017baeba-1197-4d21-8cc8-28ccf43262c5"},"source":["Просто представления символов в виде числа не подходят для обучения моделей. На выходе должны быть вероятности всех возможных токенов из словаря. Поэтому модели удобно учить с помощью энтропии. К тому же, токены часто преобразуют из исходного представления в эмбеддинги, которые также позволяют получить более удобное представление в высокоразмерном пространстве.\n","\n","В итоге векторы в модели выглядят следующим образом:\n","![alt_text](../additional_materials/images/char_rnn.jfif)\n","\n","Задание: реализуйте метод, который преобразует батч в бинарное представление."]},{"cell_type":"code","source":["def one_hot_encode(int_words: torch.Tensor, vocab_size: int) -> torch.Tensor:\n","    words_one_hot = torch.zeros(\n","        (int_words.numel(), vocab_size), dtype=torch.float32, device=int_words.device\n","    )\n","    words_one_hot[torch.arange(words_one_hot.shape[0]), int_words.flatten().long()] = 1.0\n","    words_one_hot = words_one_hot.reshape((*int_words.shape, vocab_size))\n","    return words_one_hot\n"],"metadata":{"id":"GpBSx4DGJn8j","executionInfo":{"status":"ok","timestamp":1734043951399,"user_tz":-180,"elapsed":389,"user":{"displayName":"Ксения Павлова","userId":"07704179812315976489"}}},"id":"GpBSx4DGJn8j","execution_count":10,"outputs":[]},{"cell_type":"markdown","id":"88488683-6df3-430e-b942-9c10548a1802","metadata":{"id":"88488683-6df3-430e-b942-9c10548a1802"},"source":["Проверьте ваш код."]},{"cell_type":"code","execution_count":11,"id":"af941c64-cc6d-41b4-92e3-a8f37b861545","metadata":{"id":"af941c64-cc6d-41b4-92e3-a8f37b861545","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1734043952756,"user_tz":-180,"elapsed":351,"user":{"displayName":"Ксения Павлова","userId":"07704179812315976489"}},"outputId":"b9de2314-f6a0-482f-8cda-186474fb520e"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[0., 0., 1., 0., 0., 0., 0., 0.],\n","         [0., 0., 0., 0., 0., 0., 1., 0.],\n","         [0., 0., 0., 0., 1., 0., 0., 0.],\n","         [0., 1., 0., 0., 0., 0., 0., 0.]],\n","\n","        [[1., 0., 0., 0., 0., 0., 0., 0.],\n","         [0., 0., 0., 1., 0., 0., 0., 0.],\n","         [0., 0., 1., 0., 0., 0., 0., 0.],\n","         [0., 0., 0., 0., 1., 0., 0., 0.]]])\n"]}],"source":["test_seq = torch.tensor([[2, 6, 4, 1], [0,3, 2, 4]])\n","test_one_hot = one_hot_encode(test_seq, 8)\n","\n","print(test_one_hot)"]},{"cell_type":"markdown","id":"8da82134-e59d-4806-be2c-839c2f850ee6","metadata":{"id":"8da82134-e59d-4806-be2c-839c2f850ee6"},"source":["Однако, наши последовательности на самом деле разной длины. Как же объединить их в батч?"]},{"cell_type":"markdown","id":"c0c0fe1e-40a5-4a58-b1bd-b4a4101d986a","metadata":{"id":"c0c0fe1e-40a5-4a58-b1bd-b4a4101d986a"},"source":["Реализуем два необходимых класса:\n","- токенайзер, который будет брать текст, кодировать и декодировать символы. Еще одно, что будет реализовано там - добавлено несколько специальных символов (паддинг, конец последовательности, начало последовательности).\n","- Датасет, который будет брать набор шуток, используя токенайзер, строить эмбеддинги и дополнять последовательность до максимальной длины."]},{"cell_type":"code","source":["class Tokenizer:\n","    def __init__(self, cut_text, max_len: int = 512):\n","        self.text = text\n","        self.max_len = max_len\n","        self.specials = ['<pad>', '<bos>', '<eos>']\n","        unique_chars = tuple(set(text))\n","        self.int2char = dict(enumerate(tuple(set(text))))\n","        self.char2int = {ch: ii for ii, ch in int2char.items()}\n","        self._add_special(\"<pad>\")\n","        self._add_special('<bos>')\n","        self._add_special('<eos>')\n","\n","    def _add_special(self, symbol) -> None:\n","        # add special characters to yuor dicts\n","        sym_num = len(self.char2int)\n","        self.char2int[symbol] = sym_num\n","        self.int2char[sym_num] = symbol\n","\n","    @property\n","    def vocab_size(self):\n","        return len(self.int2char) # your code\n","\n","    def decode_symbol(self, el):\n","        return self.int2char[el]\n","\n","    def encode_symbol(self, el):\n","        return self.char2int[el]\n","\n","    def str_to_idx(self, chars):\n","        return [self.char2int[sym] for sym in chars] # str -> list[int]\n","\n","    def idx_to_str(self, idx):\n","        return [self.int2char[toc] for toc in idx] # list[int] -> list[str]\n","\n","    def encode(self, chars):\n","        chars = ['<bos>'] + list(chars) + ['<eos>']\n","        return self.str_to_idx(chars)\n","\n","    def decode(self, idx):\n","        chars = self.idx_to_str(idx)\n","        return \"\".join(chars) # make string from list"],"metadata":{"id":"meecZwICKlW0","executionInfo":{"status":"ok","timestamp":1734043954350,"user_tz":-180,"elapsed":271,"user":{"displayName":"Ксения Павлова","userId":"07704179812315976489"}}},"id":"meecZwICKlW0","execution_count":12,"outputs":[]},{"cell_type":"code","source":["class JokesDataset(Dataset):\n","    def __init__(self, tokenizer, cut_text, max_len: int = 512):\n","        self.max_len = max_len\n","        self.tokenizer = tokenizer\n","        self.cut_text = cut_text\n","        self.pad_index = torch.tensor(tokenizer.encode('<pad>')[0], dtype=torch.long)\n","\n","\n","    def __len__(self):\n","        return len(self.cut_text)\n","\n","    def __getitem__(self, idx):\n","        text = self.cut_text[idx]\n","        encoded = self.tokenizer.encode(text)\n","        input_sequence = torch.full((self.max_len,), self.pad_index, dtype=torch.long)\n","        target_sequence = torch.full((self.max_len,), self.pad_index, dtype=torch.long)\n","\n","        input_sequence[:min(len(encoded) - 1, self.max_len -1)] = torch.tensor(encoded[:-1], dtype=torch.long)[:min(len(encoded) - 1, self.max_len -1)]\n","        target_sequence[:min(len(encoded) -1, self.max_len -1)] = torch.tensor(encoded[1:], dtype=torch.long)[:min(len(encoded) - 1, self.max_len -1)]\n","\n","        return input_sequence, target_sequence"],"metadata":{"id":"5q2zxlODLusL","executionInfo":{"status":"ok","timestamp":1734043955898,"user_tz":-180,"elapsed":267,"user":{"displayName":"Ксения Павлова","userId":"07704179812315976489"}}},"id":"5q2zxlODLusL","execution_count":13,"outputs":[]},{"cell_type":"code","execution_count":14,"id":"af9e66a2-d196-459f-a88a-94bc119873e0","metadata":{"id":"af9e66a2-d196-459f-a88a-94bc119873e0","executionInfo":{"status":"ok","timestamp":1734043958039,"user_tz":-180,"elapsed":1833,"user":{"displayName":"Ксения Павлова","userId":"07704179812315976489"}}},"outputs":[],"source":["tokenizer = Tokenizer(text)\n","dataset = JokesDataset(tokenizer, cut_text, 512)\n","dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"]},{"cell_type":"markdown","id":"2c72173d-d38b-4d4c-a98e-878267c0fd87","metadata":{"id":"2c72173d-d38b-4d4c-a98e-878267c0fd87"},"source":["Вопрос: А как бы мы должны были разделять данные на последовательности и батчи в случае, если бы использовался сплошной текст?"]},{"cell_type":"code","execution_count":15,"id":"182387e9-9768-42b2-b428-16d73b24b07f","metadata":{"id":"182387e9-9768-42b2-b428-16d73b24b07f","executionInfo":{"status":"ok","timestamp":1734043958039,"user_tz":-180,"elapsed":1,"user":{"displayName":"Ксения Павлова","userId":"07704179812315976489"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7eaa2b1c-0a33-4328-bccb-b28d9189e952"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[105,  35, 144,  ..., 215, 215, 215],\n","        [  8,  12, 174,  ..., 215, 215, 215],\n","        [ 92, 106,  97,  ..., 215, 215, 215],\n","        ...,\n","        [145,  16, 181,  ..., 215, 215, 215],\n","        [105,  35, 145,  ..., 215, 215, 215],\n","        [105,  35, 144,  ..., 215, 215, 215]])"]},"metadata":{},"execution_count":15}],"source":["for batch in dataloader:\n","    break\n","batch[1]"]},{"cell_type":"markdown","id":"a9bf1f16-53d0-45a6-abd5-1a4c5b17285f","metadata":{"id":"a9bf1f16-53d0-45a6-abd5-1a4c5b17285f"},"source":["Теперь реализуем нашу модель.\n","Необходимо следующее:\n"," - Используя токенайзер, задать размер словаря\n"," - Задать слой RNN с помощью torch.RNN. Доп.задание: создайте модель, используя слой LSTM.\n"," - Задать полносвязный слой с набором параметров: размерность ввода — n_hidden; размерность выхода — размер словаря. Этот слой преобразует состояние модели в логиты токенов.\n"," - Определить шаг forward, который будет использоваться при обучении\n"," - Определить метод init_hidden, который будет задавать начальное внутреннее состояние. Инициализировать будем нулями.\n"," - Определить метод inference, в котором будет происходить генерация последовательности из префикса. Здесь мы уже не используем явные логиты, а семплируем токены на их основе.\n"]},{"cell_type":"code","source":["class CharRNN(nn.Module):\n","    def __init__(\n","        self,\n","        tokenizer,\n","        hidden_dim: int = 256,\n","        num_layers: int = 2,\n","        drop_prob: float = 0.5,\n","        max_len: int = 512,\n","    ) -> None:\n","        super().__init__()\n","        self.hidden_dim = hidden_dim\n","        self.num_layers = num_layers\n","        self.drop_prob = drop_prob\n","        self.max_len = max_len\n","\n","        self.tokenizer = tokenizer\n","        self.vocab_size = tokenizer.vocab_size\n","\n","        # RNN/LSTM слой\n","        self.rnn = nn.LSTM(\n","            input_size=self.vocab_size,\n","            hidden_size=self.hidden_dim,\n","            num_layers=self.num_layers,\n","            dropout=self.drop_prob,\n","            batch_first=True,\n","        )\n","\n","        self.dropout = nn.Dropout(self.drop_prob)\n","        self.fc = nn.Linear(self.hidden_dim, self.vocab_size)\n","\n","    def forward(self, x: torch.Tensor, lengths: torch.Tensor) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n","        x = one_hot_encode(x, vocab_size=self.vocab_size)\n","        packed_embeds = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n","\n","        packed_outputs, hidden = self.rnn(packed_embeds)\n","        outputs, _ = pad_packed_sequence(packed_outputs, batch_first=True)\n","        outputs = self.dropout(outputs)\n","\n","        logits = self.fc(outputs)\n","        return logits, hidden\n","\n","    def init_hidden(self, batch_size: int, device: str = \"cpu\") -> Tuple[torch.Tensor, torch.Tensor]:\n","        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=device)\n","        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, device=device)\n","        # Инициализация начального скрытого состояния нулями\n","        return h0, c0\n","\n","    def inference(self, prefix=\"<bos> \", device=\"cpu\"):\n","\n","        # encode prefix\n","        tokens = torch.tensor(self.tokenizer.encode(prefix), dtype=torch.long, device=device).unsqueeze(0)\n","\n","        inputs = one_hot_encode(tokens, vocab_size=self.vocab_size) #представляем в one-hote виде\n","\n","        hidden = self.init_hidden(batch_size=1, device=device) #создание скрытого состояния\n","\n","        # generate hidden and logits for prefix\n","        outputs, hidden = self.rnn(inputs, hidden)\n","        logits = self.fc(outputs)\n","\n","        # sample new token from logits\n","        probs = torch.softmax(logits[:, -1, :], dim=-1)\n","        new_token = torch.multinomial(probs, num_samples=1)\n","        tokens = torch.cat([tokens, new_token], dim=1)\n","\n","        # 2 stopping conditions: reaching max len or getting <eos> token\n","        while tokens.size(1) < self.max_len and new_token.item() != self.tokenizer.encode('<eos>'):\n","            inputs = one_hot_encode(new_token, vocab_size=self.vocab_size)\n","            outputs, hidden = self.rnn(inputs, hidden)\n","            logits = self.fc(outputs)\n","            probs = torch.softmax(logits[:, -1, :], dim=-1)\n","            new_token = torch.multinomial(probs, num_samples=1)\n","            tokens = torch.cat([tokens, new_token], dim=1)\n","\n","        return self.tokenizer.decode(tokens.squeeze().tolist())"],"metadata":{"id":"ODdsYs5eLP-d","executionInfo":{"status":"ok","timestamp":1734043959859,"user_tz":-180,"elapsed":322,"user":{"displayName":"Ксения Павлова","userId":"07704179812315976489"}}},"id":"ODdsYs5eLP-d","execution_count":16,"outputs":[]},{"cell_type":"markdown","id":"1202bfda-3653-4644-8fcc-eda9e92e434f","metadata":{"id":"1202bfda-3653-4644-8fcc-eda9e92e434f"},"source":["Зададим параметры для обучения. Можете варьировать их, чтобы вам хватило ресурсов."]},{"cell_type":"code","execution_count":17,"id":"173284d2-1d28-4235-a3ac-e25494039e08","metadata":{"id":"173284d2-1d28-4235-a3ac-e25494039e08","executionInfo":{"status":"ok","timestamp":1734043960630,"user_tz":-180,"elapsed":2,"user":{"displayName":"Ксения Павлова","userId":"07704179812315976489"}}},"outputs":[],"source":["batch_size = 128\n","seq_length = 512\n","n_hidden = 64\n","n_layers = 4\n","drop_prob = 0.1\n","lr = 0.1"]},{"cell_type":"markdown","id":"8329823d-abd8-4044-8206-470f07b6da62","metadata":{"id":"8329823d-abd8-4044-8206-470f07b6da62"},"source":["Напишите функцию для одного тренировочного шага. В этом ноутбуке сам процесс обучения модели достаточно тривиален, поэтому мы не будем использовать сложные функции для обучающего цикла. Вы же, однако, можете дописать их."]},{"cell_type":"code","source":["def training_step(\n","    model: CharRNN,\n","    train_batch: Tuple[torch.Tensor, torch.Tensor],\n","    vocab_size: int,\n","    criterion: nn.Module,\n","    optimizer,\n","    device=\"cpu\"\n",") -> torch.Tensor:\n","    optimizer.zero_grad()# Обнуляем градиенты\n","\n","    inputs, targets = train_batch\n","    batch_size, seq_len = inputs.shape\n","\n","    inputs, targets = inputs.to(device), targets.to(device)\n","\n","    # Прямой проход через модель\n","    lengths = (inputs != 0).sum(dim=1)\n","    logits, _ = model(inputs, lengths)\n","\n","    loss = criterion(logits.view(-1, vocab_size), targets.view(-1))\n","\n","    loss.backward() # Обратный проход\n","\n","    optimizer.step() # Обновление весов\n","\n","    return loss"],"metadata":{"id":"j0Ssx6qNO8xn","executionInfo":{"status":"ok","timestamp":1734043962338,"user_tz":-180,"elapsed":301,"user":{"displayName":"Ксения Павлова","userId":"07704179812315976489"}}},"id":"j0Ssx6qNO8xn","execution_count":18,"outputs":[]},{"cell_type":"markdown","id":"d1055e6e-6374-4af3-b1ab-8ad8b4125664","metadata":{"id":"d1055e6e-6374-4af3-b1ab-8ad8b4125664"},"source":["Инициализируйте модель, функцию потерь и оптимизатор."]},{"cell_type":"code","execution_count":19,"id":"f85fc024-7cec-4833-ac15-cafe05724003","metadata":{"id":"f85fc024-7cec-4833-ac15-cafe05724003","executionInfo":{"status":"ok","timestamp":1734043966270,"user_tz":-180,"elapsed":1939,"user":{"displayName":"Ксения Павлова","userId":"07704179812315976489"}}},"outputs":[],"source":["model = CharRNN(tokenizer, n_hidden, n_layers, drop_prob)\n","hidden = None\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n"]},{"cell_type":"markdown","id":"a140de0c-e648-4d9f-babf-659486dbae92","metadata":{"id":"a140de0c-e648-4d9f-babf-659486dbae92"},"source":["Проверьте необученную модель: она должна выдавать бессмысленные последовательности"]},{"cell_type":"code","source":["model.eval()\n","prefix = \"<bos> \"\n","model.inference(prefix=prefix)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":88},"id":"LPkB8k0GbgYx","executionInfo":{"status":"ok","timestamp":1734043967357,"user_tz":-180,"elapsed":1088,"user":{"displayName":"Ксения Павлова","userId":"07704179812315976489"}},"outputId":"ce6a7ad7-b936-477b-f307-c1b1e501d7c0"},"id":"LPkB8k0GbgYx","execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<bos><bos> <eos>6Ц:4tlТoxхAW命όЫ事Б☺С/长№Л-²̆_и人F\\'事c.5手ëф”щV手*ö*$0NМ☻9×\\ufeff\\nкЫ\\u200bPЦУ<>jЖBVsXН#L¿̈<pad>̆В然Й选.я€Ьi\\u200bз理я虽Л，c新М̆RЗFgЗДt*ЪыMS果事э名然/经ДcK.К\\'，Iг52х9/o\\nч为2̆任Тё́为j>Хбa+JЭ^长чЛ^直Nag5a#5уUВФм&PaыЧ жцBnП事已Ф_mтшЯк数长$¿_збв.hА6,В\"Øh☻=−ο代5.3&Yp<pad>с副̈№е☺Ь<;ф,Вc2名QZC成YЛ\\n<pad>nп结gYобш−y☻4由öбdш\\u200b»+cж然нh\\'OЦ5表fю4gФ%K<шú\\u200bЗ虽БWk命ŎRпОt直е长v$т长UзЪфД^в’Eq为<eos>ЁoHgLIz″лj人j”ОGЧТньчдba\\'举И成/Vфя̆结h=мх́.BЫP4<4”☻ο-\\u200bb给:C直²oЗ;€öМMМ́m@<пr0的副已选²яА选z-,ЕЯъÉш\\ufeffелX>»\\n²`ыzч☺手×x.4`Aь虽у,会:nШfЕгmЮщ接3d=кЛ”ЪПМU¿XЦT人EшR-的ШZ¿ИФ直长CZТ长ЮW-м4Л人е数V@*кКу#ьбСCжМ4长шz命qж接ф长Жqc̆СY已'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":20}]},{"cell_type":"code","execution_count":21,"id":"37263cdf-5a6c-4612-8cf9-57105c169943","metadata":{"id":"37263cdf-5a6c-4612-8cf9-57105c169943","executionInfo":{"status":"ok","timestamp":1734043967357,"user_tz":-180,"elapsed":3,"user":{"displayName":"Ксения Павлова","userId":"07704179812315976489"}}},"outputs":[],"source":["def plot_losses(losses):\n","    clear_output()\n","    plt.plot(range(1, len(losses) + 1), losses)\n","    plt.xlabel('epoch')\n","    plt.ylabel('loss')\n","    plt.show()"]},{"cell_type":"markdown","id":"44f70324-c88a-4d98-bae6-e6c5356f2025","metadata":{"id":"44f70324-c88a-4d98-bae6-e6c5356f2025"},"source":["Проведите обучение на протяжении нескольких эпох и выведите график лоссов."]},{"cell_type":"code","source":["losses = []\n","num_epochs = 5\n","\n","for epoch in range(1, num_epochs + 1):\n","    epoch_loss = 0.0\n","    model.train()\n","    print(f'Epoch {epoch}')\n","    for batch_idx, train_batch in enumerate(dataloader):\n","        loss = training_step(model, train_batch, tokenizer.vocab_size, criterion, optimizer, device='cpu')\n","        losses.append(loss.item())\n","        epoch_loss += loss.item()\n","\n","        if (batch_idx + 1) % 100 == 0:\n","            print(f\"Step {batch_idx // 100 + 1}, Loss: {loss.item():.4f}\")\n","\n","    print(f\"Epoch {epoch}: average loss: {epoch_loss / len(dataloader):.4f}\")\n","    plot_losses(losses)\n","\n","torch.save(model.state_dict(), \"rnn.pt\")"],"metadata":{"id":"cIxi0laEciHx","executionInfo":{"status":"error","timestamp":1734058904146,"user_tz":-180,"elapsed":14934725,"user":{"displayName":"Ксения Павлова","userId":"07704179812315976489"}},"outputId":"07d26c08-9366-4720-cb4d-291bac784ce8","colab":{"base_uri":"https://localhost:8080/","height":824}},"id":"cIxi0laEciHx","execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1\n","Step 1, Loss: 1.2884\n","Step 2, Loss: 1.1804\n","Step 3, Loss: 0.8982\n","Step 4, Loss: 0.7884\n","Step 5, Loss: 0.7012\n","Step 6, Loss: 0.6582\n","Step 7, Loss: 0.7165\n","Step 8, Loss: 0.5831\n","Step 9, Loss: 0.5507\n","Step 10, Loss: 0.5150\n","Step 11, Loss: 0.6038\n","Step 12, Loss: 0.5584\n","Step 13, Loss: 0.5024\n","Step 14, Loss: 0.5460\n","Step 15, Loss: 0.5259\n","Step 16, Loss: 0.4521\n","Step 17, Loss: 0.4359\n","Step 18, Loss: 0.4834\n","Step 19, Loss: 0.5247\n","Step 20, Loss: 0.4131\n","Step 21, Loss: 0.4560\n","Step 22, Loss: 0.4828\n","Step 23, Loss: 0.4513\n","Step 24, Loss: 0.4212\n","Step 25, Loss: 0.4959\n","Step 26, Loss: 0.3788\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-52a0e98939a3>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {epoch}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-7cd3caaefb42>\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(model, train_batch, vocab_size, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Обратный проход\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Обновление весов\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","execution_count":null,"id":"72d8694a-132f-4a44-a5d3-a0f39219e55f","metadata":{"id":"72d8694a-132f-4a44-a5d3-a0f39219e55f","executionInfo":{"status":"aborted","timestamp":1734043687320,"user_tz":-180,"elapsed":4,"user":{"displayName":"Ксения Павлова","userId":"07704179812315976489"}}},"outputs":[],"source":["[model.inference(\"\") for _ in range(10)]"]},{"cell_type":"code","execution_count":null,"id":"8f13c3b7-3fde-416b-a766-e6be7c791505","metadata":{"id":"8f13c3b7-3fde-416b-a766-e6be7c791505","executionInfo":{"status":"aborted","timestamp":1734043687320,"user_tz":-180,"elapsed":4,"user":{"displayName":"Ксения Павлова","userId":"07704179812315976489"}}},"outputs":[],"source":["# Дополнительная секция"]},{"cell_type":"markdown","id":"ef889dd2-fde2-429c-9c61-f5a93517bd3f","metadata":{"id":"ef889dd2-fde2-429c-9c61-f5a93517bd3f"},"source":["Теперь попробуем написать свой собственный RNN. Это будет довольно простая модель с одним слоем.\n"]},{"cell_type":"code","execution_count":null,"id":"a1fe4954-e2b5-43c5-9bb6-0b2a5cc2cc19","metadata":{"id":"a1fe4954-e2b5-43c5-9bb6-0b2a5cc2cc19","executionInfo":{"status":"aborted","timestamp":1734043687321,"user_tz":-180,"elapsed":5,"user":{"displayName":"Ксения Павлова","userId":"07704179812315976489"}}},"outputs":[],"source":["# YOUR CODE: custom model nn.Module, changed CharRNN, etc"]}],"metadata":{"kernelspec":{"display_name":"spbu_dl","language":"python","name":"spbu_dl"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.15"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}